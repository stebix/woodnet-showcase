{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64fe84d2",
   "metadata": {},
   "source": [
    "# Extended Showcase: Explanations and Visualization\n",
    "# Prediction\n",
    "\n",
    "\n",
    "This notebook is intended to serve as an extended visualization and explanation document for our paper manuscript. We aim to show insights about the deep-learning based softwood and hardwood wood classification. For this, we provide two sample data chunks from both classes. These sample data chunks are downloaded from the open-source data sharing platform Zenodo via\n",
    "\n",
    "<a href=\"https://doi.org/10.5281/zenodo.13905949\"><img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.13905949.svg\" alt=\"DOI\"></a>\n",
    "\n",
    "In the following, the raw data flow towards a model-ingestible format is shown. Further, we actually perform the classification using a sample model weights artifact.\n",
    "All this demonstrates the core forward/prediction workflow for a model in PyTorch.\n",
    "If you want to apply/utilize novel data of your possession with the `woodnet`, please note that the package facilitates more advanced and more convenient functionality to perform predictions and test the model robustness.\n",
    "\n",
    "Note: To actually run the ntoebook code live in your browser, select 'Run' $\\rightarrow$ 'Run All Cells' in the drop down menu bar.\n",
    "Since we are running on public resources, please allow up to 90 seconds runtime for the resource-intensive model prediction cells.\n",
    "\n",
    "\n",
    "2024, Jannik Stebani\n",
    "\n",
    "IAWA Paper Manuscript by Jannik Stebani, Tim Lewandrowski, Kilian Dremel, Simon Zabler and Volker Haag: \"Technical framework presentation of advanced volumetric sub-Î¼-CT imaging integrated with adaptable deep learning-based wood species classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de357f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from rich import print as pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3521fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bfc2d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# woodnet imports\n",
    "import woodnet\n",
    "from woodnet.configtools import load_yaml\n",
    "from woodnet.models import create_model\n",
    "from woodnet.datasets.reader import read_data_from_hdf5, read_fingerprint_from_hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1bd2cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we read all input data from\n",
    "ARTIFACT_DIR = pathlib.Path('./artifacts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a16f45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for zarr and HDF5, this internal path can be used to bundle versions of the same dataset in a single file-like object\n",
    "INTERNAL_PATH: str = 'scangroup/dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2485b156",
   "metadata": {},
   "source": [
    "Now we need to load the data. The first step is inputting paths and ensuring the actual presence on the file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4494fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACER_ARTIFACT_PATH = ARTIFACT_DIR / 'acer-artifact.hdf5'\n",
    "assert ACER_ARTIFACT_PATH.is_file(), 'setup failed: acer data artifact missing'\n",
    "PINUS_ARTIFACT_PATH = ARTIFACT_DIR / 'pinus-artifact.hdf5'\n",
    "assert PINUS_ARTIFACT_PATH.is_file(), 'setup failed: pinus data artifact missing'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e6483",
   "metadata": {},
   "source": [
    "Now to acutally loading the data into the main memory. We can use the reader interface provided by the `woodnet` package for that.\n",
    "See e.g. [Reader API Docs](https://woodnet.readthedocs.io/en/latest/api/datasets/reader.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "988a51d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "acer_data, acer_fingerprint = read_data_from_hdf5(ACER_ARTIFACT_PATH, INTERNAL_PATH), read_fingerprint_from_hdf5(ACER_ARTIFACT_PATH, INTERNAL_PATH)\n",
    "pinus_data, pinus_fingerprint = read_data_from_hdf5(PINUS_ARTIFACT_PATH, INTERNAL_PATH), read_fingerprint_from_hdf5(PINUS_ARTIFACT_PATH, INTERNAL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e6011a",
   "metadata": {},
   "source": [
    "We can now take a closer look at the data, i.e. inspect the shape, the fingerprint and some descriptive parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b013256",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(acer_fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(f'acer artifact shape: {acer_data.shape}')\n",
    "pprint(f'acer artifact min: {acer_data.min():.3f}')\n",
    "pprint(f'acer artifact max: {acer_data.max():.3f}')\n",
    "pprint(f'acer artifact mean: {acer_data.mean():.3f}')\n",
    "pprint(f'acer artifact standard deviation: {acer_data.std(ddof=1):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9ac0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(f'pinus artifact shape: {pinus_data.shape}')\n",
    "pprint(f'pinus artifact min: {pinus_data.min():.3f}')\n",
    "pprint(f'pinus artifact max: {pinus_data.max():.3f}')\n",
    "pprint(f'pinus artifact mean: {pinus_data.mean():.3f}')\n",
    "pprint(f'pinus artifact standard deviation: {pinus_data.std(ddof=1):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1be046",
   "metadata": {},
   "source": [
    "Neat! We got a single subvolume chunk of both classes to use. With this, we can perform an exemplary prediction to show how the data gets into the model at the most basic level.\n",
    "For the next step, we instantiate the model and load it with some pretrained weights. First we set up an instance of the core volumetric ResNet model\n",
    "through the configuration and `create_model` convenience function.\n",
    "Note that the weights were result of a separate training process. Due to the nature of the deep learning process as a not fully deterministic process (random seed, stochastic gradient descent, weight initialization, etc.) we may not get the exact numbers as detailed in the manuscript and for other training experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7d57f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = {'model' : {'name': 'ResNet3D', 'in_channels': 1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e3298d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cf496e",
   "metadata": {},
   "source": [
    "The training run was done with the class to label map. Shown below. I.e. the class 'acer' was given the numerical label 0 and class 'pinus' got the numerical label 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e0aab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "classlabel_mapping = {'acer' : 0, 'pinus' : 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f1e592",
   "metadata": {},
   "source": [
    "We can inspect the mode by looking at its pretty-printed string representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57364d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61fd765",
   "metadata": {},
   "source": [
    "The above output looks good! A fully featured volumetric ResNet model. The next step involves the loading of the pretrained weights.\n",
    "We supplied on instance of those in the data repository at Zenodo, also downloaded locally to here.\n",
    "We can use torch for this. The state dict was originally produced on the GPU, thus we need to set the loading target location to CPU in the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "911588d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_WEIGHTS_PATH = ARTIFACT_DIR / 'weights-artifact.pth'\n",
    "assert PRETRAINED_WEIGHTS_PATH.is_file(), 'setup failed: pretrained weights artifact missing' \n",
    "\n",
    "state_dict = torch.load(PRETRAINED_WEIGHTS_PATH, map_location='cpu', weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee735609",
   "metadata": {},
   "source": [
    "The follwing code loads the trained model weights into the network object. Optimally, the output should read `<All keys matched successfully>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec34690",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79a6c06",
   "metadata": {},
   "source": [
    "To set the final nonlinearity to active and to configure the norm layers correctly, we have to enter eval and testing mode for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa32d4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.testing = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ea98fa",
   "metadata": {},
   "source": [
    "Generally, we push the data in the format $(N \\times C \\times D \\times H \\times W)$ through. In the woodnet pipeline, this is generally taken car of. Here, we have to supplement two leading dummy dimension to ensure conistency and compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "636b85cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "acer_tensor = torch.from_numpy(\n",
    "    acer_data[np.newaxis, np.newaxis, ...]\n",
    ")\n",
    "pinus_tensor = torch.from_numpy( \n",
    "    pinus_data[np.newaxis, np.newaxis, ...]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507de479",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    pprint(f'prediction score for acer class: {model(acer_tensor).item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752eae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    pprint(f'prediction score for acer class: {model(pinus_tensor).item():.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86a713c",
   "metadata": {},
   "source": [
    "Considering the classlabel mapping from above (i.e. 'acer' $\\rightarrow$ 0 and 'pinus' $\\rightarrow$ 1), this is the optimal, expected and desired result.\n",
    "Below, we also provide a small visualization of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362c6919",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "axes = axes.flat\n",
    "vmin = -3.0\n",
    "vmax = +3.0\n",
    "kwargs = {'vmin' : vmin, 'vmax' : vmax}\n",
    "\n",
    "ax = axes[0]\n",
    "ax.set_title('Acer Artifact')\n",
    "ax.imshow(acer_tensor.numpy()[0, 0, 128, ...], **kwargs)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.set_title('Pinus Artifact')\n",
    "ax.imshow(pinus_tensor.numpy()[0, 0, 128, ...], **kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
